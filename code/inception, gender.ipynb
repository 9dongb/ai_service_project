{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 학습, 시험 테이터 셋 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 316 images belonging to 4 classes.\n",
      "Found 76 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "training_dir = 'imgs/face_training'\n",
    "validation_dir = 'imgs/face_validation'\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    # Inception V3 입력 크기 150*150\n",
    "    target_size=(150, 150),\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    # Inception V3 입력 크기 150*150\n",
    "    target_size=(150, 150),\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inceptionV3 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib\n",
    "\n",
    "\n",
    "# weights_url = 'https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "# weights_file = 'models/inception_v3.h5'\n",
    "\n",
    "# # weights_url에서 파일을 가져와 weights_file이라는 이름으로 저장\n",
    "# urllib.request.urlretrieve(weights_url, weights_file)\n",
    "\n",
    "weights_file = 'models/inception_v3.h5'\n",
    "\n",
    "# Inception V3 신경망 모델 생성\n",
    "pre_trained_model = tf.keras.applications.inception_v3.InceptionV3(\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False,\n",
    "    weights=None)\n",
    "\n",
    "# 생성된 모델에 가져온 가중치 부여\n",
    "pre_trained_model.load_weights(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정보 출력\n",
    "pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rnehd\\.conda\\envs\\cap\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 424ms/step - acc: 0.2594 - loss: 3.0736 - val_acc: 0.5263 - val_loss: 1.1863\n",
      "Epoch 2/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 304ms/step - acc: 0.4254 - loss: 1.2613 - val_acc: 0.5000 - val_loss: 0.9849\n",
      "Epoch 3/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 301ms/step - acc: 0.5073 - loss: 1.0808 - val_acc: 0.7105 - val_loss: 0.8018\n",
      "Epoch 4/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 299ms/step - acc: 0.6510 - loss: 0.9019 - val_acc: 0.6579 - val_loss: 0.7953\n",
      "Epoch 5/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 297ms/step - acc: 0.6707 - loss: 0.8388 - val_acc: 0.4737 - val_loss: 1.0021\n",
      "Epoch 6/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 296ms/step - acc: 0.5787 - loss: 0.9668 - val_acc: 0.7237 - val_loss: 0.6670\n",
      "Epoch 7/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 297ms/step - acc: 0.7233 - loss: 0.6947 - val_acc: 0.7105 - val_loss: 0.9335\n",
      "Epoch 8/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 284ms/step - acc: 0.6676 - loss: 0.8917 - val_acc: 0.6711 - val_loss: 0.7944\n",
      "Epoch 9/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 297ms/step - acc: 0.6781 - loss: 0.7915 - val_acc: 0.7632 - val_loss: 0.5617\n",
      "Epoch 10/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 306ms/step - acc: 0.7370 - loss: 0.6877 - val_acc: 0.7368 - val_loss: 0.6249\n",
      "Epoch 11/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 296ms/step - acc: 0.7500 - loss: 0.6364 - val_acc: 0.7368 - val_loss: 0.7955\n",
      "Epoch 12/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 291ms/step - acc: 0.7799 - loss: 0.6087 - val_acc: 0.7763 - val_loss: 0.6344\n",
      "Epoch 13/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 306ms/step - acc: 0.7354 - loss: 0.6390 - val_acc: 0.8026 - val_loss: 0.5534\n",
      "Epoch 14/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 290ms/step - acc: 0.7806 - loss: 0.5632 - val_acc: 0.7500 - val_loss: 0.6967\n",
      "Epoch 15/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 318ms/step - acc: 0.7333 - loss: 0.6462 - val_acc: 0.7763 - val_loss: 0.8220\n",
      "Epoch 16/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 557ms/step - acc: 0.7974 - loss: 0.5420 - val_acc: 0.6974 - val_loss: 0.7856\n",
      "Epoch 17/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 631ms/step - acc: 0.8023 - loss: 0.5326 - val_acc: 0.8553 - val_loss: 0.4561\n",
      "Epoch 18/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 288ms/step - acc: 0.7889 - loss: 0.4927 - val_acc: 0.7763 - val_loss: 0.5229\n",
      "Epoch 19/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 298ms/step - acc: 0.7999 - loss: 0.4880 - val_acc: 0.7763 - val_loss: 0.4805\n",
      "Epoch 20/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 293ms/step - acc: 0.7941 - loss: 0.5565 - val_acc: 0.7632 - val_loss: 0.6091\n",
      "Epoch 21/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 320ms/step - acc: 0.8396 - loss: 0.4426 - val_acc: 0.7368 - val_loss: 0.6872\n",
      "Epoch 22/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 318ms/step - acc: 0.7737 - loss: 0.6060 - val_acc: 0.7895 - val_loss: 0.6269\n",
      "Epoch 23/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 335ms/step - acc: 0.8346 - loss: 0.4693 - val_acc: 0.8421 - val_loss: 0.3967\n",
      "Epoch 24/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 324ms/step - acc: 0.8527 - loss: 0.3684 - val_acc: 0.7763 - val_loss: 0.6152\n",
      "Epoch 25/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 319ms/step - acc: 0.8559 - loss: 0.4245 - val_acc: 0.7895 - val_loss: 0.5517\n",
      "Epoch 26/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 640ms/step - acc: 0.8542 - loss: 0.4294 - val_acc: 0.8026 - val_loss: 0.6461\n",
      "Epoch 27/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 374ms/step - acc: 0.7914 - loss: 0.5032 - val_acc: 0.8684 - val_loss: 0.4902\n",
      "Epoch 28/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 322ms/step - acc: 0.8464 - loss: 0.3763 - val_acc: 0.8553 - val_loss: 0.4129\n",
      "Epoch 29/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 327ms/step - acc: 0.8535 - loss: 0.3833 - val_acc: 0.8026 - val_loss: 0.4375\n",
      "Epoch 30/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 330ms/step - acc: 0.8025 - loss: 0.4972 - val_acc: 0.8553 - val_loss: 0.4285\n",
      "Epoch 31/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 315ms/step - acc: 0.8094 - loss: 0.4437 - val_acc: 0.7895 - val_loss: 0.5119\n",
      "Epoch 32/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 327ms/step - acc: 0.8677 - loss: 0.3390 - val_acc: 0.7895 - val_loss: 0.5471\n",
      "Epoch 33/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 389ms/step - acc: 0.8054 - loss: 0.4498 - val_acc: 0.8421 - val_loss: 0.3500\n",
      "Epoch 34/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 737ms/step - acc: 0.8461 - loss: 0.3206 - val_acc: 0.7763 - val_loss: 0.6840\n",
      "Epoch 35/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 749ms/step - acc: 0.8398 - loss: 0.3714 - val_acc: 0.7632 - val_loss: 0.5854\n",
      "Epoch 36/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 273ms/step - acc: 0.8516 - loss: 0.3370 - val_acc: 0.8026 - val_loss: 0.4787\n",
      "Epoch 37/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 344ms/step - acc: 0.8575 - loss: 0.3532 - val_acc: 0.8816 - val_loss: 0.5441\n",
      "Epoch 38/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 335ms/step - acc: 0.8729 - loss: 0.3435 - val_acc: 0.8684 - val_loss: 0.6134\n",
      "Epoch 39/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 330ms/step - acc: 0.8516 - loss: 0.3610 - val_acc: 0.8289 - val_loss: 0.4422\n",
      "Epoch 40/40\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 339ms/step - acc: 0.9048 - loss: 0.2694 - val_acc: 0.8553 - val_loss: 0.4436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x202a935f310>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가져온 신경망의 parameter가 훈련되지 않도록 동결\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# mixed7 층의 마지막 출력을 가리키는 변수 생성\n",
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "last_output = last_layer.output\n",
    "\n",
    "# 출력 펼치기\n",
    "x = tf.keras.layers.Flatten()(last_output)\n",
    "# Dense 층 추가\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "# 다중 분류를 위해 소프트맥스 함수를 사용한 출력층 생성\n",
    "x = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "# 모델 생성\n",
    "model = tf.keras.Model(pre_trained_model.input, x)\n",
    "\n",
    "# 훈련 방법 설정\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# 훈련 시작\n",
    "model.fit(train_generator, epochs=40, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('models/inceptionV3_celebrity.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.5434474e-04, 1.0179839e-01, 1.4838825e-01, 7.4915904e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 이미지 로드\n",
    "image = cv2.imread('imgs/predict/GuDongBin_faces/bin.jpg')\n",
    "\n",
    "# # 이미지 크기 조정 (150, 150)으로 조정\n",
    "# image = cv2.resize(image, (150, 150))\n",
    "\n",
    "# 모델이 기대하는 형태로 차원 추가 (batch 차원 추가)\n",
    "input_image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# 이미지를 0과 1사이의 값으로 조정 (정규화)\n",
    "input_image = input_image/255.0\n",
    "\n",
    "# 입력 이미지에 대한 예측\n",
    "model.predict(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "당신이 서강준일 확률은 14.84% 입니다!\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(input_image)\n",
    "\n",
    "# 학습시켰던 연예인 이름 레이블\n",
    "class_labels = ['이승기', '남주혁', '박보영', '서강준']\n",
    "\n",
    "# 가장 큰 원소의 인덱스를 반환\n",
    "predict_class_index = np.argmax(pred)\n",
    "\n",
    "# 가장 높은 확률을 가진 클래스의 레이블명 반환\n",
    "predict_label = class_labels[predict_class_index]\n",
    "\n",
    "print(f'당신이 {predict_label}일 확률은 {round(pred[0][predict_class_index]*100, 2)}% 입니다!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
