{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T11:27:23.445174Z",
     "start_time": "2024-04-11T11:27:23.342978Z"
    }
   },
   "source": [
    "import pathlib\n",
    "import uuid\n",
    "from collections import Counter\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 초기 전처리\n",
    "\n",
    "나온 얼굴만 뽑아내기 (얼굴 여러개 포함)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T11:27:58.724133Z",
     "start_time": "2024-04-11T11:27:58.698980Z"
    }
   },
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "\n",
    "def face_extraction(image, mtcnn):\n",
    "    # image = np.array(image)\n",
    "\n",
    "    # Check the number of dimensions in the image\n",
    "    if len(image.shape) == 3 and image.shape[2] == 4:\n",
    "        # Convert the image from RGBA to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(image)\n",
    "\n",
    "    faces = []\n",
    "    if boxes is not None and len(boxes) > 0:\n",
    "        for i, box in enumerate(boxes):\n",
    "            box = [int(b) for b in box]\n",
    "            face = image[box[1]: box[3], box[0]: box[2]]\n",
    "            faces.append(face)\n",
    "    return faces\n",
    "\n",
    "\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "face_counter = Counter()\n",
    "\n",
    "name_list = [\"이승기\", \"남주혁\", \"박보영\", \"서강준\"]\n",
    "\n",
    "for who in name_list:\n",
    "    pathlib.Path(f\"faces/{who}_faces\").mkdir(parents=True, exist_ok=True)\n",
    "    for file in pathlib.Path(f\"imgs/{who}_images\").iterdir():\n",
    "        image = cv2.imread(str(file))\n",
    "        # image = Image.open(file)\n",
    "        print(f\"processing {file}\")\n",
    "        faces = face_extraction(image, mtcnn)\n",
    "        for i, face in enumerate(faces):\n",
    "            if face is not None and face.size != 0:\n",
    "                cv2.imwrite(f\"faces/{who}_faces/{uuid.uuid4()}_{i}.png\", face)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing imgs\\이승기_images\\이승기_0.jpeg\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 36\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# image = Image.open(file)\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocessing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 36\u001B[0m faces \u001B[38;5;241m=\u001B[39m \u001B[43mface_extraction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmtcnn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, face \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(faces):\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m face \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m face\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[1;32mIn[3], line 8\u001B[0m, in \u001B[0;36mface_extraction\u001B[1;34m(image, mtcnn)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mface_extraction\u001B[39m(image, mtcnn):\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;66;03m# image = np.array(image)\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \n\u001B[0;32m      7\u001B[0m     \u001B[38;5;66;03m# Check the number of dimensions in the image\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m image\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m:\n\u001B[0;32m      9\u001B[0m         \u001B[38;5;66;03m# Convert the image from RGBA to RGB\u001B[39;00m\n\u001B[0;32m     10\u001B[0m         image \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(image, cv2\u001B[38;5;241m.\u001B[39mCOLOR_RGBA2RGB)\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# Detect faces\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 얼굴 빈도가 적은 파일 삭제 (패스함. 그대신 손으로 했음)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import face_recognition\n",
    "\n",
    "# # Load known face encodings and labels\n",
    "# known_face_encodings = ...\n",
    "# known_face_labels = ...\n",
    "\n",
    "# def face_extraction_and_recognition(image, mtcnn):\n",
    "#     # Detect faces\n",
    "#     boxes, _ = mtcnn.detect(image)\n",
    "\n",
    "#     faces = []\n",
    "#     if boxes is not None:\n",
    "#         for i, box in enumerate(boxes):\n",
    "#             box = [int(b) for b in box]\n",
    "#             face = image[box[1] : box[3], box[0] : box[2]]\n",
    "#             # Compute face encoding\n",
    "#             face_encoding = face_recognition.face_encodings(face)\n",
    "#             # Compare face encoding with known face encodings\n",
    "#             matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "#             if True in matches:\n",
    "#                 matched_label = known_face_labels[matches.index(True)]\n",
    "#                 faces.append((face, matched_label))\n",
    "#     return faces\n",
    "\n",
    "# face_counter = Counter()\n",
    "\n",
    "# for who in name_list:\n",
    "#     pathlib.Path(f\"faces/{who}_faces\").mkdir(parents=True, exist_ok=True)\n",
    "#     for file in pathlib.Path(f\"imgs/{who}_images\").iterdir():\n",
    "#         image = cv2.imread(str(file))\n",
    "#         print(f\"processing {file}\")\n",
    "#         faces = face_extraction_and_recognition(image, mtcnn)\n",
    "#         for i, (face, label) in enumerate(faces):\n",
    "#             if face is not None and face.size != 0 and label == who:\n",
    "#                 cv2.imwrite(f\"faces/{who}_faces/{uuid.uuid4()}_{i}.png\", face)\n",
    "#                 face_counter[who] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_list = [\"이승기\", \"남주혁\", \"박보영\", \"서강준\"]\n",
    "\n",
    "# # for other survived files, grayscale and resize\n",
    "# for who in name_list:\n",
    "#     pathlib.Path(f\"faces/{who}_faces_resized\").mkdir(parents=True, exist_ok=True)\n",
    "#     for file in pathlib.Path(f\"faces/{who}_faces\").iterdir():\n",
    "#         image = cv2.imread(str(file))\n",
    "#         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY, cv2.IMREAD_GRAYSCALE)\n",
    "#         resized = cv2.resize(gray, (299, 299))\n",
    "#         cv2.imwrite(f\"faces/{who}_faces_resized/{file.name}\", resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치로 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6144, 0.4978, 0.4530])\n",
      "tensor([0.0212, 0.0162, 0.0165])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load your images\n",
    "data_dir = \"faces\"\n",
    "dataset = ImageFolder(data_dir, transform=transforms.ToTensor())\n",
    "\n",
    "# Calculate the mean and std\n",
    "mean = torch.stack([t.mean(1).mean(1) for t, _ in dataset]).mean(0)\n",
    "std = torch.stack([t.std(1).std(1) for t, _ in dataset]).std(0)\n",
    "\n",
    "print(mean)\n",
    "print(std)\n",
    "# tensor([0.6144, 0.4978, 0.4530])\n",
    "# tensor([0.0212, 0.0162, 0.0165])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'남주혁_faces': 0, '박보영_faces': 1, '서강준_faces': 2, '이승기_faces': 3}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "data_dir = \"faces\"\n",
    "\n",
    "# Define your transformations\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.RandomResizedCrop(299),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(40),\n",
    "        transforms.RandomAffine(0, shear=20, scale=(0.8, 1.2)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.6144, 0.4978, 0.4530], [0.0212, 0.0162, 0.0165]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "validation_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.6144, 0.4978, 0.4530], [0.0212, 0.0162, 0.0165]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load your images and labels\n",
    "dataset = ImageFolder(data_dir, transform=train_transforms)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "# Apply the validation transformations to the validation dataset\n",
    "valid_dataset.dataset.transform = validation_transforms\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(ImageFolder(data_dir).class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4090\n",
      "Epoch: 1/40.. Training Loss: 1.583.. Validation Loss: 1.469.. Accuracy: 0.281\n",
      "Epoch: 2/40.. Training Loss: 1.290.. Validation Loss: 1.328.. Accuracy: 0.352\n",
      "Epoch: 3/40.. Training Loss: 1.177.. Validation Loss: 1.590.. Accuracy: 0.211\n",
      "Epoch: 4/40.. Training Loss: 1.104.. Validation Loss: 1.082.. Accuracy: 0.508\n",
      "Epoch: 5/40.. Training Loss: 0.975.. Validation Loss: 1.329.. Accuracy: 0.469\n",
      "Epoch: 6/40.. Training Loss: 0.970.. Validation Loss: 1.251.. Accuracy: 0.438\n",
      "Epoch: 7/40.. Training Loss: 0.847.. Validation Loss: 1.195.. Accuracy: 0.438\n",
      "Epoch: 8/40.. Training Loss: 0.793.. Validation Loss: 1.033.. Accuracy: 0.602\n",
      "Epoch: 9/40.. Training Loss: 0.723.. Validation Loss: 0.957.. Accuracy: 0.586\n",
      "Epoch: 10/40.. Training Loss: 0.680.. Validation Loss: 1.061.. Accuracy: 0.453\n",
      "Epoch: 11/40.. Training Loss: 0.682.. Validation Loss: 0.918.. Accuracy: 0.656\n",
      "Epoch: 12/40.. Training Loss: 0.560.. Validation Loss: 0.872.. Accuracy: 0.625\n",
      "Epoch: 13/40.. Training Loss: 0.611.. Validation Loss: 0.821.. Accuracy: 0.664\n",
      "Epoch: 14/40.. Training Loss: 0.659.. Validation Loss: 0.776.. Accuracy: 0.680\n",
      "Epoch: 15/40.. Training Loss: 0.558.. Validation Loss: 0.769.. Accuracy: 0.734\n",
      "Epoch: 16/40.. Training Loss: 0.545.. Validation Loss: 0.857.. Accuracy: 0.602\n",
      "Epoch: 17/40.. Training Loss: 0.648.. Validation Loss: 0.856.. Accuracy: 0.633\n",
      "Epoch: 18/40.. Training Loss: 0.576.. Validation Loss: 1.138.. Accuracy: 0.508\n",
      "Epoch: 19/40.. Training Loss: 0.731.. Validation Loss: 1.205.. Accuracy: 0.602\n",
      "Epoch: 20/40.. Training Loss: 0.552.. Validation Loss: 1.003.. Accuracy: 0.539\n",
      "Epoch: 21/40.. Training Loss: 0.745.. Validation Loss: 0.979.. Accuracy: 0.742\n",
      "Epoch: 22/40.. Training Loss: 0.597.. Validation Loss: 1.393.. Accuracy: 0.531\n",
      "Epoch: 23/40.. Training Loss: 0.541.. Validation Loss: 1.287.. Accuracy: 0.430\n",
      "Epoch: 24/40.. Training Loss: 0.677.. Validation Loss: 0.873.. Accuracy: 0.711\n",
      "Epoch: 25/40.. Training Loss: 0.588.. Validation Loss: 0.771.. Accuracy: 0.547\n",
      "Epoch: 26/40.. Training Loss: 0.549.. Validation Loss: 0.637.. Accuracy: 0.789\n",
      "Epoch: 27/40.. Training Loss: 0.527.. Validation Loss: 0.657.. Accuracy: 0.820\n",
      "Epoch: 28/40.. Training Loss: 0.364.. Validation Loss: 1.005.. Accuracy: 0.516\n",
      "Epoch: 29/40.. Training Loss: 0.460.. Validation Loss: 0.854.. Accuracy: 0.578\n",
      "Epoch: 30/40.. Training Loss: 0.424.. Validation Loss: 0.769.. Accuracy: 0.688\n",
      "Epoch: 31/40.. Training Loss: 0.436.. Validation Loss: 0.807.. Accuracy: 0.648\n",
      "Epoch: 32/40.. Training Loss: 0.488.. Validation Loss: 0.723.. Accuracy: 0.773\n",
      "Epoch: 33/40.. Training Loss: 0.463.. Validation Loss: 0.609.. Accuracy: 0.805\n",
      "Epoch: 34/40.. Training Loss: 0.406.. Validation Loss: 0.683.. Accuracy: 0.789\n",
      "Epoch: 35/40.. Training Loss: 0.450.. Validation Loss: 0.812.. Accuracy: 0.648\n",
      "Epoch: 36/40.. Training Loss: 0.332.. Validation Loss: 0.676.. Accuracy: 0.703\n",
      "Epoch: 37/40.. Training Loss: 0.367.. Validation Loss: 0.737.. Accuracy: 0.680\n",
      "Epoch: 38/40.. Training Loss: 0.421.. Validation Loss: 0.581.. Accuracy: 0.719\n",
      "Epoch: 39/40.. Training Loss: 0.454.. Validation Loss: 0.943.. Accuracy: 0.625\n",
      "Epoch: 40/40.. Training Loss: 0.586.. Validation Loss: 0.733.. Accuracy: 0.578\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import models\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained model\n",
    "model = models.inception_v3(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the last layer\n",
    "num_classes = len(dataset.classes)  # 4\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(model.fc.in_features, 1024),\n",
    "    nn.RReLU(),\n",
    "    nn.Dropout(0.128),\n",
    "    nn.Linear(1024, num_classes),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.016)\n",
    "\n",
    "# Train the model\n",
    "epochs = 40\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = (\n",
    "            inputs.to(device),\n",
    "            labels.to(device),\n",
    "        )  # Move inputs and labels to device\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = (\n",
    "                inputs.to(device),\n",
    "                labels.to(device),\n",
    "            )  # Move inputs and labels to device\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            ps = torch.exp(outputs)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch + 1}/{epochs}.. Training Loss: {train_loss / len(train_loader):.3f}.. Validation Loss: {valid_loss / len(valid_loader):.3f}.. Accuracy: {accuracy / len(valid_loader):.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.61%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the number of correct predictions\n",
    "correct_preds = 0\n",
    "total_preds = 0\n",
    "\n",
    "# No need to track gradients for validation, we're not optimizing.\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the predicted class with the highest score\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        # Count number of correct predictions\n",
    "        correct_preds += (predictions == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_preds / total_preds\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6831, 0.0360, 0.1716, 0.1092]], device='cuda:0')\n",
      "당신이 남주혁일 확률은 68.31% 입니다.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.6144, 0.4978, 0.4530], [0.0212, 0.0162, 0.0165]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(\"/home/ubuntu/ai_service_project2/app/uploads/NamJoohyuk17.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n",
    "image = Image.fromarray(image)\n",
    "\n",
    "# Apply the transformations\n",
    "input_image = transform(image)\n",
    "\n",
    "# Add an extra dimension for the batch\n",
    "input_image = input_image.unsqueeze(0)\n",
    "\n",
    "# Move the input to the same device as the model\n",
    "input_image = input_image.to(device)\n",
    "\n",
    "# Make the prediction\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(input_image)\n",
    "    predictions = torch.exp(outputs)\n",
    "\n",
    "# Get the class labels\n",
    "class_labels = [\"남주혁\", \"박보영\", \"서강준\", \"이승기\"]\n",
    "\n",
    "# Get the index of the class with the highest probability\n",
    "_, predict_class_index = torch.max(predictions, 1)\n",
    "\n",
    "# Get the name of the class\n",
    "predict_label = class_labels[predict_class_index]\n",
    "print(predictions)\n",
    "\n",
    "print(\n",
    "    f\"당신이 {predict_label}일 확률은 {predictions[0][predict_class_index].item() * 100:.2f}% 입니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(\"./model\").mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model, \"./model/model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
